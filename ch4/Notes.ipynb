{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 Transfer Learning with BERT\n",
    "\n",
    "- Overview of transfer learning and use in NLP\n",
    "- Loading pre-trained GloVe embeddings in a model\n",
    "- Building a sentiment analysis model using pre-trained GloVe embeddings and fine-tuning\n",
    "- Overview of contextual embeddings using Attention â€“ BERT\n",
    "- Loading pre-trained BERT models using the Hugging Face library\n",
    "- Using pre-trained and custom BERT-based fine-tuned models for sentiment analysis\n",
    "\n",
    "## Transfer learning overview\n",
    "\n",
    "Transfer learning is a core concept that has made rapid advances in NLP possible. We will discuss transfer learning first.\n",
    "\n",
    "with transfer learning we are talking the pre-trained apply to some knowledge and fine tunned agannst target.\n",
    "\n",
    "### Types of transfer learning\n",
    "The concepts of domains and tasks underpin the concept of transfer learning\n",
    "- domain: represents a specific area of knowledge or data\n",
    "- task: is a specific objective or action within a domain\n",
    "\n",
    "### Domain adaptation\n",
    "having same source and target tasks\n",
    "the difference are related to the distribution of training and test data.\n",
    "some story about tank in woods and different light\n",
    "\n",
    "### Multi Task learning\n",
    "In multi-task learning, data from different but related tasks are passed through a set of common layers. Then, there may be task-specific layers on the top that learn about a particular task objective.\n",
    "\n",
    "Writing, drawing, and painting can be considered different tasks that share a standard \"layer\" of holding a pen or pencil\n",
    "\n",
    "### Sequential learning\n",
    "Sequential learning is the most common form of transfer learning. It is named so because it involves two simple steps executed in sequence. The first step is pre-training and the second step is fine-tuning. \n",
    "\n",
    "The weights learned by the pre-trained model can be frozen during the training of the task-specific model, or those weights can be updated or fine-tuned. When the weights are frozen, then this pattern of using the pre-trained model is called feature extraction.\n",
    "\n",
    "## IMDb sentiment analysis with GloVe embeddings\n",
    "\n",
    "### GloVe embeddings\n",
    "\n",
    ">the training took so long i need to find out why my GPU is not been picked up!!!!\n",
    "\n",
    ">finsihed at Train a Fine-Tuning Sequential Transfer Learning Model taking to long to execute.\n",
    "\n",
    ">Found link that showed how to get tensorflow to work with GPU, it has made a massive difference.\n",
    "\n",
    "Even with the GPU the fine-tuning bert on imdb took 24 hours and looked like it was no where near finished.\n",
    "\n",
    "GloVe is similar to Word2Vec with the addtion of holding on to the frequencies of word occurances for the entire document.\n",
    "\n",
    "[Note]\n",
    "To minimize the chances of not finding matches between the pre-trained vocabulary and task-specific vocabulary, ensure that similar tokenization schemes are used\n",
    "\n",
    "- Load data\n",
    "- normalization/preprocess the data (convert to lower case, etc before)\n",
    "- Tokenize the data \n",
    "- load and pre-train\n",
    "- create a pretrained embedding matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Feature Extraction Model\n",
    "\n",
    "Has pre trained weights and does not update the weight\n",
    "An important issue with this approach if there is any words that have not been vectorised "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
