{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T08:07:07.448778Z",
     "start_time": "2020-10-01T08:07:05.522580Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional GPU Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T08:07:08.320660Z",
     "start_time": "2020-10-01T08:07:07.452701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "######## GPU CONFIGS FOR RTX 2070 ###############\n",
    "## Please ignore if not training on GPU       ##\n",
    "## this is important for running CuDNN on GPU ##\n",
    "\n",
    "tf.keras.backend.clear_session() #- for easy reset of notebook state\n",
    "\n",
    "# chck if GPU can be seen by TF\n",
    "tf.config.list_physical_devices('GPU')\n",
    "# only if you want to see how commands are executed\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  print(gpus[0])\n",
    "  try:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading IMDB Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T08:07:08.413980Z",
     "start_time": "2020-10-01T08:07:08.322420Z"
    }
   },
   "outputs": [],
   "source": [
    "# using TFDS dataset\n",
    "# note: as_supervised converts dicts to tuples\n",
    "imdb_train, ds_info = tfds.load(name=\"imdb_reviews\", split=\"train\", \n",
    "                                with_info=True, as_supervised=True)\n",
    "imdb_test = tfds.load(name=\"imdb_reviews\", split=\"test\", \n",
    "                      as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T08:07:08.624146Z",
     "start_time": "2020-10-01T08:07:08.457259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string) \n",
      " tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Check and example from the dataset\n",
    "for example, label in imdb_train.take(1):\n",
    "    print(example, '\\n', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Vocab and Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T08:07:18.839943Z",
     "start_time": "2020-10-01T08:07:11.885640Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the default tokenizer settings\n",
    "tokenizer =  tfds.deprecated.text.Tokenizer()\n",
    "\n",
    "vocabulary_set = set()\n",
    "MAX_TOKENS = 0\n",
    "\n",
    "for example, label in imdb_train:\n",
    "  some_tokens = tokenizer.tokenize(example.numpy())\n",
    "  if MAX_TOKENS < len(some_tokens):\n",
    "        MAX_TOKENS = len(some_tokens)\n",
    "  vocabulary_set.update(some_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T08:07:19.041154Z",
     "start_time": "2020-10-01T08:07:18.868924Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93931 2525\n"
     ]
    }
   ],
   "source": [
    "imdb_encoder = tfds.deprecated.text.TokenTextEncoder(vocabulary_set,\n",
    "                                                   lowercase=True,\n",
    "                                                   tokenizer=tokenizer)\n",
    "vocab_size = imdb_encoder.vocab_size\n",
    "\n",
    "print(vocab_size, MAX_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T08:07:19.106359Z",
     "start_time": "2020-10-01T08:07:19.069590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string)\n",
      "this was an absolutely terrible movie don t be lured in by christopher walken or michael ironside both are great actors but this must simply be their worst role in history even their great acting could not redeem this movie s ridiculous storyline this movie is an early nineties us propaganda piece the most pathetic scenes were those when the columbian rebels were making their cases for revolutions maria conchita alonso appeared phony and her pseudo love affair with walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning i am disappointed that there are movies like this ruining actor s like christopher walken s good name i could barely sit through it\n"
     ]
    }
   ],
   "source": [
    "# Lets verify tokenization and encoding works\n",
    "for example, label in imdb_train.take(1):\n",
    "    print(example)\n",
    "    encoded = imdb_encoder.encode(example.numpy())\n",
    "    print(imdb_encoder.decode(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T08:07:19.141403Z",
     "start_time": "2020-10-01T08:07:19.135630Z"
    }
   },
   "outputs": [],
   "source": [
    "# transformation functions to be used with the dataset\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "def encode_pad_transform(sample):\n",
    "    encoded = imdb_encoder.encode(sample.numpy())\n",
    "    pad = sequence.pad_sequences([encoded], padding='post', \n",
    "                                 maxlen=150)\n",
    "    return np.array(pad[0], dtype=np.int64)  \n",
    "\n",
    "\n",
    "def encode_tf_fn(sample, label):\n",
    "    encoded = tf.py_function(encode_pad_transform, \n",
    "                                       inp=[sample], \n",
    "                                       Tout=(tf.int64))\n",
    "    encoded.set_shape([None])\n",
    "    label.set_shape([])\n",
    "    return encoded, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T08:07:22.122486Z",
     "start_time": "2020-10-01T08:07:22.053702Z"
    }
   },
   "outputs": [],
   "source": [
    "# test the transformation on a small subset\n",
    "subset = imdb_train.take(10)\n",
    "tst = subset.map(encode_tf_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T08:07:22.596678Z",
     "start_time": "2020-10-01T08:07:22.495265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[71889 53896 87286 61224 31423 93816 83872 29236 82262 72896 93005 49907\n",
      " 34944 93666 93861 64795 75364 62875 86463 42012 93720 87790 71889 69810\n",
      " 65423 82262 87862 44540 77730 93005 89207 46454 87862 42012 73958 57972\n",
      " 93316 31890 71889 93816 52772 88294 59375 71889 93816 56440 87286 38190\n",
      " 89696 61627 77655 59597 60590 69640 42005 88620 71233 59160 87358 60590\n",
      " 67883 75870 71233 73483 87862 33798 77114 51310 93150  2213 47463 83310\n",
      " 91271 66853 73371 50067 78905 82644 83228 93666 53896 29343 87790 86451\n",
      " 42005 81385 69563 93005 86451 93816 86166 53896 39105 55275 64408 78300\n",
      " 92827 62363 83420 65896 86166 46272 86463 87360 84277 71889 69380 70593\n",
      " 52772 84277 34944 93666 52772 79005 54279 62363 57972 80057 77095 50819\n",
      " 49351     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0], shape=(150,), dtype=int64) tf.Tensor(0, shape=(), dtype=int64)\n",
      "this was an absolutely terrible movie don t be lured in by christopher walken or michael ironside both are great actors but this must simply be their worst role in history even their great acting could not redeem this movie s ridiculous storyline this movie is an early nineties us propaganda piece the most pathetic scenes were those when the columbian rebels were making their cases for revolutions maria conchita alonso appeared phony and her pseudo love affair with walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning i am disappointed that there are movies like this ruining actor s like christopher walken s good name i could barely sit through it\n"
     ]
    }
   ],
   "source": [
    "for review, label in tst.take(1):\n",
    "    print(review, label)\n",
    "    print(imdb_encoder.decode(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T08:07:25.519970Z",
     "start_time": "2020-10-01T08:07:25.497699Z"
    }
   },
   "outputs": [],
   "source": [
    "# now tokenize/encode/pad all training\n",
    "# and testing data\n",
    "\n",
    "\n",
    "\n",
    "encoded_train = imdb_train.map(encode_tf_fn,\n",
    "                               num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "encoded_test = imdb_test.map(encode_tf_fn,\n",
    "                             num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T07:59:05.529056Z",
     "start_time": "2020-05-10T07:59:05.525541Z"
    }
   },
   "source": [
    "# GloVe Based Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Trained GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T06:01:01.276440Z",
     "start_time": "2020-05-09T05:54:20.645242Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download the GloVe embeddings\n",
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:36:33.061400Z",
     "start_time": "2020-10-01T07:36:12.366889Z"
    }
   },
   "outputs": [],
   "source": [
    "#!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T08:07:37.261596Z",
     "start_time": "2020-10-01T08:07:30.537789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary Size:  400000\n"
     ]
    }
   ],
   "source": [
    "# We will use the small 50D GLoVe vectors for this example\n",
    "# It should be trivial to experiment with a different size\n",
    "\n",
    "dict_w2v = {}\n",
    "with open('../data/glove/glove.6B.50d.txt', encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        tokens = line.split()\n",
    "        word = tokens[0]\n",
    "        vector = np.array(tokens[1:], dtype=np.float32)\n",
    "\n",
    "        if vector.shape[0] == 50:\n",
    "            dict_w2v[word] = vector\n",
    "        else:\n",
    "            print(\"There was an issue with \" + word)\n",
    "\n",
    "# lets check the vocabulary size\n",
    "print(\"Dictionary Size: \", len(dict_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import stanza\n",
    "##en = stanza.download('en') \n",
    "#en = stanza.Pipeline(lang='en')\n",
    "#dict_w2v = {}\n",
    "#with open('../data/glove/glove.6B.50d.txt', encoding=\"utf8\") as file:\n",
    "#    for line in file:\n",
    "#        tokenized = en(line)\n",
    "#        for snt in tokenized.sentences:\n",
    "#            for word in snt.tokens:\n",
    "#                print(snt.tokens)\n",
    "#                #vector = np.array(snt.tokens[1:], dtype=np.float32)\n",
    "#                #dict_w2v[word] = vector\n",
    "        \n",
    "\n",
    "## lets check the vocabulary size\n",
    "#print(\"Dictionary Size: \", len(dict_w2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an Embedding Matrix\n",
    "For each token in the training and test data, we will look up\n",
    "the corresponding token in the GloVe dictionary loaded above.\n",
    "Then, a mapping between the embeddings and token IDs will be \n",
    "created. This `embedding matrix` will be passed to the embedding\n",
    "layer of the model as the pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T08:12:14.289656Z",
     "start_time": "2020-10-01T08:12:14.284533Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "embedding_matrix = np.zeros((imdb_encoder.vocab_size, embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T08:16:52.134969Z",
     "start_time": "2020-10-01T08:16:51.463633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unknown words:  14553\n"
     ]
    }
   ],
   "source": [
    "unk_cnt = 0\n",
    "unk_set = set()\n",
    "for word in imdb_encoder.tokens:\n",
    "    embedding_vector = dict_w2v.get(word)\n",
    "\n",
    "    if embedding_vector is not None:\n",
    "        tkn_id = imdb_encoder.encode(word)[0]\n",
    "        embedding_matrix[tkn_id] = embedding_vector\n",
    "    else:\n",
    "        unk_cnt += 1\n",
    "        unk_set.add(word)\n",
    "\n",
    "# Print how many werent found\n",
    "print(\"Total unknown words: \", unk_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe based BiLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T08:21:28.580684Z",
     "start_time": "2020-10-01T08:21:28.577783Z"
    }
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = imdb_encoder.vocab_size # len(chars)\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 64\n",
    "\n",
    "#batch size\n",
    "BATCH_SIZE=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T08:26:06.221751Z",
     "start_time": "2020-10-01T08:26:06.215360Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, \\\n",
    "                                    Bidirectional, Dense,\\\n",
    "                                    Dropout\n",
    "            \n",
    "def build_model_bilstm(vocab_size, embedding_dim, \n",
    "                       rnn_units, batch_size, train_emb=False):\n",
    "  model = tf.keras.Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, mask_zero=True,\n",
    "              weights=[embedding_matrix], trainable=train_emb),\n",
    "    #Dropout(0.25),  \n",
    "    Bidirectional(tf.keras.layers.LSTM(rnn_units, return_sequences=True, \n",
    "                                      dropout=0.5)),\n",
    "    Bidirectional(tf.keras.layers.LSTM(rnn_units, dropout=0.25)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Feature Extraction Sequential Transfer Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T08:30:47.250665Z",
     "start_time": "2020-10-01T08:30:43.965844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 50)          4696550   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, None, 128)        58880     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 128)              98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,854,375\n",
      "Trainable params: 157,825\n",
      "Non-trainable params: 4,696,550\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_fe = build_model_bilstm(\n",
    "  vocab_size = vocab_size,\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "model_fe.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T08:35:26.189297Z",
     "start_time": "2020-10-01T08:35:26.066180Z"
    }
   },
   "outputs": [],
   "source": [
    "model_fe.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T19:01:21.644426Z",
     "start_time": "2020-10-01T19:01:21.638858Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prefetch for performance\n",
    "encoded_train_batched = encoded_train.batch(BATCH_SIZE).prefetch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T08:55:06.446716Z",
     "start_time": "2020-10-01T08:44:43.172092Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ParallelMapDataset shapes: ((None,), ()), types: (tf.int64, tf.int64)>\n",
      "<PrefetchDataset shapes: ((None, None), (None,)), types: (tf.int64, tf.int64)>\n",
      "Epoch 1/10\n",
      "250/250 [==============================] - 106s 265ms/step - loss: 0.6002 - accuracy: 0.6658 - precision: 0.6672 - recall: 0.66181s - loss: 0.6104 - accu\n",
      "Epoch 2/10\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 0.5227 - accuracy: 0.7414 - precision: 0.7438 - recall: 0.7363\n",
      "Epoch 3/10\n",
      "250/250 [==============================] - 70s 280ms/step - loss: 0.4907 - accuracy: 0.7618 - precision: 0.7656 - recall: 0.7545\n",
      "Epoch 4/10\n",
      "250/250 [==============================] - 75s 299ms/step - loss: 0.4586 - accuracy: 0.7851 - precision: 0.7918 - recall: 0.7736\n",
      "Epoch 5/10\n",
      "250/250 [==============================] - 76s 304ms/step - loss: 0.4451 - accuracy: 0.7913 - precision: 0.7942 - recall: 0.786512s - loss: 0.4469 - accur\n",
      "Epoch 6/10\n",
      "250/250 [==============================] - 72s 287ms/step - loss: 0.4307 - accuracy: 0.8029 - precision: 0.8068 - recall: 0.7966\n",
      "Epoch 7/10\n",
      "250/250 [==============================] - 66s 265ms/step - loss: 0.4221 - accuracy: 0.8065 - precision: 0.8085 - recall: 0.8034\n",
      "Epoch 8/10\n",
      "250/250 [==============================] - 59s 234ms/step - loss: 0.4085 - accuracy: 0.8142 - precision: 0.8171 - recall: 0.8095\n",
      "Epoch 9/10\n",
      "250/250 [==============================] - 58s 231ms/step - loss: 0.4003 - accuracy: 0.8153 - precision: 0.8193 - recall: 0.8091\n",
      "Epoch 10/10\n",
      "250/250 [==============================] - 58s 231ms/step - loss: 0.3976 - accuracy: 0.8177 - precision: 0.8227 - recall: 0.8098\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x280493f1d60>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(encoded_train)\n",
    "print(encoded_train_batched)\n",
    "\n",
    "model_fe.fit(encoded_train_batched, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14054450616154680703\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4851040256\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 17146640924310264810\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name(): \n",
    "\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "\n",
    "else:\n",
    "\n",
    "   print(\"Please install GPU version of TF\")\n",
    "\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T09:00:06.757789Z",
     "start_time": "2020-10-01T08:59:43.318698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 54s 168ms/step - loss: 0.3959 - accuracy: 0.8356 - precision: 0.8003 - recall: 0.894234s - loss: 0.4148 - accuracy: 0.8239 - precision: 0.7851 - recall - ETA - ETA: 28s - loss: 0.4111 - accuracy: 0.8278 - precision: 0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.39589136838912964,\n",
       " 0.8355600237846375,\n",
       " 0.8003150224685669,\n",
       " 0.8942400217056274]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fe.evaluate(encoded_test.batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_fe.evaluate([\"not very good\",\"loved this\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Train a Fine-Tuning Sequential Transfer Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T04:17:36.305596Z",
     "start_time": "2020-05-12T04:17:33.539387Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 50)          4696550   \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, None, 128)        58880     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 128)              98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,854,375\n",
      "Trainable params: 4,854,375\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_ft = build_model_bilstm(\n",
    "  vocab_size=vocab_size,\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  train_emb=True)\n",
    "\n",
    "model_ft.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T04:17:36.442748Z",
     "start_time": "2020-05-12T04:17:36.307825Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_ft.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T04:26:27.201294Z",
     "start_time": "2020-05-12T04:17:36.444668Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "250/250 [==============================] - 77s 219ms/step - loss: 0.5549 - accuracy: 0.7056 - precision: 0.7088 - recall: 0.6980\n",
      "Epoch 2/50\n",
      "250/250 [==============================] - 59s 236ms/step - loss: 0.4033 - accuracy: 0.8192 - precision: 0.8183 - recall: 0.8205\n",
      "Epoch 3/50\n",
      "250/250 [==============================] - 58s 231ms/step - loss: 0.3294 - accuracy: 0.8590 - precision: 0.8599 - recall: 0.8578\n",
      "Epoch 4/50\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.2735 - accuracy: 0.8863 - precision: 0.8843 - recall: 0.8888\n",
      "Epoch 5/50\n",
      "250/250 [==============================] - 45s 180ms/step - loss: 0.2377 - accuracy: 0.9045 - precision: 0.9056 - recall: 0.9031\n",
      "Epoch 6/50\n",
      "250/250 [==============================] - 44s 174ms/step - loss: 0.2082 - accuracy: 0.9170 - precision: 0.9174 - recall: 0.9165\n",
      "Epoch 7/50\n",
      "250/250 [==============================] - 43s 172ms/step - loss: 0.1783 - accuracy: 0.9318 - precision: 0.9344 - recall: 0.928910s - loss: 0\n",
      "Epoch 8/50\n",
      "250/250 [==============================] - 44s 174ms/step - loss: 0.1515 - accuracy: 0.9420 - precision: 0.9441 - recall: 0.9397\n",
      "Epoch 9/50\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.1318 - accuracy: 0.9516 - precision: 0.9535 - recall: 0.9494\n",
      "Epoch 10/50\n",
      "250/250 [==============================] - 43s 174ms/step - loss: 0.1092 - accuracy: 0.9598 - precision: 0.9615 - recall: 0.9581\n",
      "Epoch 11/50\n",
      "250/250 [==============================] - 44s 174ms/step - loss: 0.0946 - accuracy: 0.9646 - precision: 0.9654 - recall: 0.9638\n",
      "Epoch 12/50\n",
      "250/250 [==============================] - 43s 172ms/step - loss: 0.0827 - accuracy: 0.9706 - precision: 0.9711 - recall: 0.9701\n",
      "Epoch 13/50\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.0724 - accuracy: 0.9748 - precision: 0.9748 - recall: 0.9747\n",
      "Epoch 14/50\n",
      "250/250 [==============================] - 45s 181ms/step - loss: 0.0654 - accuracy: 0.9770 - precision: 0.9782 - recall: 0.9757\n",
      "Epoch 15/50\n",
      "250/250 [==============================] - 45s 179ms/step - loss: 0.0554 - accuracy: 0.9818 - precision: 0.9823 - recall: 0.9812\n",
      "Epoch 16/50\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.0470 - accuracy: 0.9833 - precision: 0.9835 - recall: 0.9830\n",
      "Epoch 17/50\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.0448 - accuracy: 0.9843 - precision: 0.9849 - recall: 0.9838\n",
      "Epoch 18/50\n",
      "250/250 [==============================] - 43s 172ms/step - loss: 0.0419 - accuracy: 0.9854 - precision: 0.9857 - recall: 0.9850\n",
      "Epoch 19/50\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.0380 - accuracy: 0.9868 - precision: 0.9880 - recall: 0.98550s - loss: 0.0380 - accuracy: 0.9868 - precision: 0.9880 - recall: 0.985\n",
      "Epoch 20/50\n",
      "250/250 [==============================] - 44s 174ms/step - loss: 0.0321 - accuracy: 0.9892 - precision: 0.9895 - recall: 0.9888\n",
      "Epoch 21/50\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.0279 - accuracy: 0.9904 - precision: 0.9906 - recall: 0.9902\n",
      "Epoch 22/50\n",
      "250/250 [==============================] - 44s 174ms/step - loss: 0.0246 - accuracy: 0.9916 - precision: 0.9920 - recall: 0.9913\n",
      "Epoch 23/50\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.0238 - accuracy: 0.9919 - precision: 0.9924 - recall: 0.9914\n",
      "Epoch 24/50\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.0233 - accuracy: 0.9920 - precision: 0.9919 - recall: 0.9922\n",
      "Epoch 25/50\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.0203 - accuracy: 0.9930 - precision: 0.9933 - recall: 0.9927\n",
      "Epoch 26/50\n",
      "250/250 [==============================] - 44s 174ms/step - loss: 0.0188 - accuracy: 0.9932 - precision: 0.9938 - recall: 0.9926\n",
      "Epoch 27/50\n",
      "250/250 [==============================] - 43s 172ms/step - loss: 0.0210 - accuracy: 0.9922 - precision: 0.9923 - recall: 0.9920\n",
      "Epoch 28/50\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.0167 - accuracy: 0.9940 - precision: 0.9942 - recall: 0.9938\n",
      "Epoch 29/50\n",
      "250/250 [==============================] - 44s 174ms/step - loss: 0.0173 - accuracy: 0.9939 - precision: 0.9947 - recall: 0.9930\n",
      "Epoch 30/50\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.0179 - accuracy: 0.9939 - precision: 0.9939 - recall: 0.9938\n",
      "Epoch 31/50\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.0167 - accuracy: 0.9944 - precision: 0.9950 - recall: 0.9938\n",
      "Epoch 32/50\n",
      "250/250 [==============================] - 43s 172ms/step - loss: 0.0165 - accuracy: 0.9942 - precision: 0.9942 - recall: 0.9942\n",
      "Epoch 33/50\n",
      "250/250 [==============================] - 43s 172ms/step - loss: 0.0123 - accuracy: 0.9959 - precision: 0.9961 - recall: 0.9958\n",
      "Epoch 34/50\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.0107 - accuracy: 0.9960 - precision: 0.9956 - recall: 0.9963\n",
      "Epoch 35/50\n",
      "250/250 [==============================] - 44s 174ms/step - loss: 0.0096 - accuracy: 0.9964 - precision: 0.9963 - recall: 0.9966\n",
      "Epoch 36/50\n",
      "250/250 [==============================] - 43s 172ms/step - loss: 0.0098 - accuracy: 0.9964 - precision: 0.9966 - recall: 0.9963\n",
      "Epoch 37/50\n",
      "250/250 [==============================] - 44s 174ms/step - loss: 0.0092 - accuracy: 0.9972 - precision: 0.9972 - recall: 0.9971\n",
      "Epoch 38/50\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.0075 - accuracy: 0.9970 - precision: 0.9972 - recall: 0.99693s - loss: 0.0075 - accuracy: 0.9971 - precision: 0.99\n",
      "Epoch 39/50\n",
      "250/250 [==============================] - 43s 172ms/step - loss: 0.0089 - accuracy: 0.9968 - precision: 0.9968 - recall: 0.9967\n",
      "Epoch 40/50\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.0064 - accuracy: 0.9978 - precision: 0.9976 - recall: 0.9980\n",
      "Epoch 41/50\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.0066 - accuracy: 0.9980 - precision: 0.9980 - recall: 0.9981\n",
      "Epoch 42/50\n",
      "250/250 [==============================] - 43s 172ms/step - loss: 0.0069 - accuracy: 0.9976 - precision: 0.9977 - recall: 0.9975\n",
      "Epoch 43/50\n",
      "250/250 [==============================] - 44s 174ms/step - loss: 0.0069 - accuracy: 0.9974 - precision: 0.9973 - recall: 0.9974\n",
      "Epoch 44/50\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.0099 - accuracy: 0.9962 - precision: 0.9959 - recall: 0.9965\n",
      "Epoch 45/50\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 0.0060 - accuracy: 0.9978 - precision: 0.9978 - recall: 0.9978\n",
      "Epoch 46/50\n",
      "250/250 [==============================] - 44s 174ms/step - loss: 0.0067 - accuracy: 0.9978 - precision: 0.9978 - recall: 0.9977\n",
      "Epoch 47/50\n",
      "250/250 [==============================] - 44s 175ms/step - loss: 0.0069 - accuracy: 0.9975 - precision: 0.9972 - recall: 0.9978\n",
      "Epoch 48/50\n",
      "250/250 [==============================] - 43s 172ms/step - loss: 0.0054 - accuracy: 0.9982 - precision: 0.9980 - recall: 0.9983\n",
      "Epoch 49/50\n",
      "250/250 [==============================] - 44s 174ms/step - loss: 0.0050 - accuracy: 0.9982 - precision: 0.9985 - recall: 0.9980\n",
      "Epoch 50/50\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.0048 - accuracy: 0.9983 - precision: 0.9983 - recall: 0.9982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22e28829c10>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft.fit(encoded_train_batched, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T04:26:52.706549Z",
     "start_time": "2020-05-12T04:26:29.231956Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 44s 147ms/step - loss: 1.1449 - accuracy: 0.8402 - precision: 0.8490 - recall: 0.827618s - loss: 1.1725 - accuracy: - ETA: 15s - loss: 1.1746 - accuracy: 0.8396 - precision: 0.8468 - recall:  - ETA: 15s - loss: 1.1764 - accuracy: 0.8393 - precis - ETA: 13s - loss: 1.17 - ETA: 7s - loss: 1.1596 - accuracy: 0.8388 - precision: 0 - ETA: 4s - loss: 1.1482 - accuracy: 0.8403 - preci\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1449368000030518,\n",
       " 0.8402000069618225,\n",
       " 0.8489946722984314,\n",
       " 0.8276000022888184]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft.evaluate(encoded_test.batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Based Transfer Learning With HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T09:04:46.583854Z",
     "start_time": "2020-10-01T09:04:46.578349Z"
    }
   },
   "outputs": [],
   "source": [
    "# To clean up and free up GPU memory\n",
    "# optional\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Normalization with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T04:32:19.722729Z",
     "start_time": "2020-05-12T04:32:17.671088Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AUTO'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#model_ft.save('../data/model_ft')\n",
    "#model_fe.save('../data/model_fe')\n",
    "\n",
    "#!pip install transformers\n",
    "#pip install transformers==3.0.2\n",
    "\n",
    "\n",
    "os.environ.get(\"USE_TORCH\", \"AUTO\").upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T09:09:26.702837Z",
     "start_time": "2020-10-01T09:09:26.067176Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T09:14:05.035558Z",
     "start_time": "2020-10-01T09:14:04.638788Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining BERT tokenizer\n",
    "#bert_name = 'bert-base-uncased'\n",
    "bert_name = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_name, add_special_tokens=True, \n",
    "                                          do_lower_case=False, max_length=150, \n",
    "                                          pad_to_max_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T18:50:05.261007Z",
     "start_time": "2020-10-01T18:50:05.251743Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1188, 1108, 1126, 7284, 6434, 2523, 119, 1790, 112, 189, 1129, 19615, 1181, 1107, 1118, 4978, 10065, 1424, 1137, 1847, 5621, 5570, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst = \"This was an absolutely terrible movie. Don't be lured in \\\n",
    "        by Christopher Walken or Michael Ironside.\"\n",
    "tokenizer.encode_plus(tst, add_special_tokens=True, max_length=150, \n",
    "                      pad_to_max_length=True, \n",
    "                      return_attention_mask=True, \n",
    "                      return_token_type_ids=True,\n",
    "                      truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T09:23:19.138143Z",
     "start_time": "2020-10-01T09:23:19.131757Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. [SEP]\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.encode(tst, add_special_tokens=True)\n",
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T09:27:57.407069Z",
     "start_time": "2020-10-01T09:27:57.402834Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] This was an absolutely terrible movie . Don ' t be lure ##d in by Christopher Walk ##en or Michael Iron ##side . [SEP]\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([tokenizer.decode([tok]) for tok in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T18:50:28.017266Z",
     "start_time": "2020-10-01T18:50:28.008805Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1790, 112, 189, 1129, 19615, 1181, 102, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(\"Don't be lured\", add_special_tokens=True, \n",
    "                      max_length=9,\n",
    "                      pad_to_max_length=True, \n",
    "                      return_attention_mask=True, \n",
    "                      return_token_type_ids=True,\n",
    "                      truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T18:50:37.807911Z",
     "start_time": "2020-10-01T18:50:37.799776Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1790, 112, 189, 1129, 102, 19615, 1181, 102, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 1, 1, 1, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(\"Don't be\",\" lured\", add_special_tokens=True, \n",
    "                      max_length=10,\n",
    "                      pad_to_max_length=True, \n",
    "                      return_attention_mask=True, \n",
    "                      return_token_type_ids=True,\n",
    "                      truncation=True\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T18:52:37.106709Z",
     "start_time": "2020-10-01T18:52:37.100036Z"
    }
   },
   "outputs": [],
   "source": [
    "def bert_encoder(review):\n",
    "    txt = review.numpy().decode('utf-8')\n",
    "    encoded = tokenizer.encode_plus(txt, add_special_tokens=True, \n",
    "                                    max_length=150, pad_to_max_length=True, \n",
    "                                    return_attention_mask=True, \n",
    "                                    return_token_type_ids=True,\n",
    "                                    truncation=True)\n",
    "    return encoded['input_ids'], encoded['token_type_ids'], \\\n",
    "           encoded['attention_mask']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T18:51:38.139815Z",
     "start_time": "2020-10-01T18:51:38.134730Z"
    }
   },
   "outputs": [],
   "source": [
    "tst = imdb_train.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T09:51:08.792340Z",
     "start_time": "2020-10-01T09:51:08.756547Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for review, label in tst:\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T18:54:26.165715Z",
     "start_time": "2020-10-01T18:52:48.450412Z"
    }
   },
   "outputs": [],
   "source": [
    "bert_train = [bert_encoder(r) for r,l in imdb_train]\n",
    "bert_lbl = [l for r, l in imdb_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T10:02:42.055253Z",
     "start_time": "2020-10-01T10:02:41.150109Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3, 150)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_train = np.array(bert_train)\n",
    "bert_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T10:07:19.748485Z",
     "start_time": "2020-10-01T10:07:19.581436Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_lbl = tf.keras.utils.to_categorical(bert_lbl, num_classes=2)\n",
    "bert_lbl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T10:11:56.611502Z",
     "start_time": "2020-10-01T10:11:56.579187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 3, 150) (20000, 2)\n"
     ]
    }
   ],
   "source": [
    "# create training and validation splits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#print(bert_train.shape, bert_lbl.shape)\n",
    "\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(bert_train, bert_lbl, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T10:16:35.683164Z",
     "start_time": "2020-10-01T10:16:35.678248Z"
    }
   },
   "outputs": [],
   "source": [
    "tr_reviews, tr_segments, tr_masks = np.split(x_train, 3, axis=1)\n",
    "val_reviews, val_segments, val_masks = np.split(x_val, 3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T10:21:13.180584Z",
     "start_time": "2020-10-01T10:21:13.176534Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 1, 150)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T10:25:50.634524Z",
     "start_time": "2020-10-01T10:25:50.629353Z"
    }
   },
   "outputs": [],
   "source": [
    "tr_reviews = tr_reviews.squeeze()\n",
    "tr_segments = tr_segments.squeeze()\n",
    "tr_masks = tr_masks.squeeze()\n",
    "\n",
    "val_reviews = val_reviews.squeeze()\n",
    "val_segments = val_segments.squeeze()\n",
    "val_masks = val_masks.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T10:30:28.540210Z",
     "start_time": "2020-10-01T10:30:28.433940Z"
    }
   },
   "outputs": [],
   "source": [
    "def example_to_features(input_ids,attention_masks,token_type_ids,y):\n",
    "  return {\"input_ids\": input_ids,\n",
    "          \"attention_mask\": attention_masks,\n",
    "          \"token_type_ids\": token_type_ids},y\n",
    "\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((tr_reviews, tr_masks, \n",
    "                                               tr_segments, y_train)).\\\n",
    "            map(example_to_features).shuffle(100).batch(16)\n",
    "\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((val_reviews, val_masks, \n",
    "                                               val_segments, y_val)).\\\n",
    "            map(example_to_features).shuffle(100).batch(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Built BERT Model for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T10:35:07.512686Z",
     "start_time": "2020-10-01T10:35:07.510310Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T17:44:51.149621Z",
     "start_time": "2020-10-01T17:44:49.523074Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_tf_utils:Some weights of the model checkpoint at bert-base-cased were not used when initializing TFBertForSequenceClassification: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_tf_utils:Some weights of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier', 'dropout_37']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Warnings can be ignored safely\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained(bert_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T17:45:00.681767Z",
     "start_time": "2020-10-01T17:45:00.661197Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "bert_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T17:45:11.451723Z",
     "start_time": "2020-10-01T17:45:11.419761Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  108310272 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108,311,810\n",
      "Trainable params: 108,311,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T18:01:29.845576Z",
     "start_time": "2020-10-01T17:45:58.992940Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning BERT on IMDB\n",
      "Epoch 1/2\n",
      "   1/1250 [..............................] - ETA: 6:31:26 - loss: 0.7625 - accuracy: 0.2500"
     ]
    }
   ],
   "source": [
    "print(\"Fine-tuning BERT on IMDB\")\n",
    "#bert_model = tf.keras.models.load_model('../data/bertmodel')\n",
    "bert_history = bert_model.fit(train_ds, epochs=2, validation_data=valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 1065). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/bertmodel\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/bertmodel\\assets\n"
     ]
    }
   ],
   "source": [
    "bert_model.save('../data/bertmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('../data/bertmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T18:41:46.327008Z",
     "start_time": "2020-10-01T18:39:33.125855Z"
    }
   },
   "outputs": [],
   "source": [
    "# prep data for testing\n",
    "bert_test = [bert_encoder(r) for r,l in imdb_test]\n",
    "bert_tst_lbl = [l for r, l in imdb_test]\n",
    "\n",
    "bert_test2 = np.array(bert_test)\n",
    "bert_tst_lbl2 = tf.keras.utils.to_categorical(bert_tst_lbl, num_classes=2)\n",
    "\n",
    "ts_reviews, ts_segments, ts_masks = np.split(bert_test2, 3, axis=1)\n",
    "ts_reviews = ts_reviews.squeeze()\n",
    "ts_segments = ts_segments.squeeze()\n",
    "ts_masks = ts_masks.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T18:45:39.081546Z",
     "start_time": "2020-10-01T18:45:39.004047Z"
    }
   },
   "outputs": [],
   "source": [
    "test_ds = tf.data.Dataset.from_tensor_slices((ts_reviews, ts_masks, \n",
    "                                               ts_segments, bert_tst_lbl2)).\\\n",
    "            map(example_to_features).shuffle(100).batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T18:48:58.090196Z",
     "start_time": "2020-10-01T18:45:48.287431Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 447s 282ms/step - loss: 0.6933 - accuracy: 0.5096\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6932675838470459, 0.5096399784088135]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Model With BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T19:02:02.903212Z",
     "start_time": "2020-10-01T19:02:02.899329Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T19:02:13.133445Z",
     "start_time": "2020-10-01T19:02:11.393478Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_tf_utils:Some weights of the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_tf_utils:All the weights of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert_name = 'bert-base-cased'\n",
    "bert = TFBertModel.from_pretrained(bert_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T19:02:28.391826Z",
     "start_time": "2020-10-01T19:02:28.359327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  108310272 \n",
      "=================================================================\n",
      "Total params: 108,310,272\n",
      "Trainable params: 108,310,272\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T19:03:08.809003Z",
     "start_time": "2020-10-01T19:03:08.795796Z"
    }
   },
   "outputs": [],
   "source": [
    "max_seq_len = 150\n",
    "inp_ids = tf.keras.layers.Input((max_seq_len,), dtype=tf.int64, name=\"input_ids\")\n",
    "att_mask = tf.keras.layers.Input((max_seq_len,), dtype=tf.int64, name=\"attention_mask\")\n",
    "seg_ids = tf.keras.layers.Input((max_seq_len,), dtype=tf.int64, name=\"token_type_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T19:03:19.420529Z",
     "start_time": "2020-10-01T19:03:19.413883Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': TensorSpec(shape=(None, 150), dtype=tf.int64, name=None),\n",
       "  'attention_mask': TensorSpec(shape=(None, 150), dtype=tf.int64, name=None),\n",
       "  'token_type_ids': TensorSpec(shape=(None, 150), dtype=tf.int64, name=None)},\n",
       " TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T19:03:27.668481Z",
     "start_time": "2020-10-01T19:03:26.105288Z"
    }
   },
   "outputs": [],
   "source": [
    "inp_dict = {\"input_ids\": inp_ids,\n",
    "          \"attention_mask\": att_mask,\n",
    "          \"token_type_ids\": seg_ids}\n",
    "outputs = bert(inp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T11:08:15.827749Z",
     "start_time": "2020-05-20T11:08:15.824369Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'tf_bert_model/Identity:0' shape=(None, 150, 768) dtype=float32>,\n",
       " <tf.Tensor 'tf_bert_model/Identity_1:0' shape=(None, 768) dtype=float32>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets see the output structure\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T19:51:13.767038Z",
     "start_time": "2020-10-01T19:51:13.595711Z"
    }
   },
   "outputs": [],
   "source": [
    "x = tf.keras.layers.Dropout(0.2)(outputs[1])\n",
    "x = tf.keras.layers.Dense(200, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = tf.keras.layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "custom_model = tf.keras.models.Model(inputs=inp_dict, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T19:42:21.293269Z",
     "start_time": "2020-10-01T19:42:21.285839Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ({input_ids: (None, 150), attention_mask: (None, 150), token_type_ids: (None, 150)}, (None, 2)), types: ({input_ids: tf.int64, attention_mask: tf.int64, token_type_ids: tf.int64}, tf.float32)>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T19:51:24.990614Z",
     "start_time": "2020-10-01T19:51:24.961479Z"
    }
   },
   "outputs": [],
   "source": [
    "# first train the new layers added\n",
    "bert.trainable = False  \n",
    "optimizer = tf.keras.optimizers.Adam()  # standard learning rate\n",
    "loss = tf.keras.losses.BinaryCrossentropy() #from_logits=True)\n",
    "custom_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T20:26:45.526004Z",
     "start_time": "2020-10-01T19:51:32.715506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Model: training custom model on IMDB\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 205s 164ms/step - loss: 0.2915 - accuracy: 0.9096 - val_loss: 0.3498 - val_accuracy: 0.8614\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 207s 165ms/step - loss: 0.2664 - accuracy: 0.9118 - val_loss: 0.3537 - val_accuracy: 0.8618\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 211s 168ms/step - loss: 0.2627 - accuracy: 0.9128 - val_loss: 0.3464 - val_accuracy: 0.8616\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 211s 169ms/step - loss: 0.2597 - accuracy: 0.9137 - val_loss: 0.3386 - val_accuracy: 0.8616\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 211s 169ms/step - loss: 0.2589 - accuracy: 0.9122 - val_loss: 0.3814 - val_accuracy: 0.8616\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 211s 169ms/step - loss: 0.2601 - accuracy: 0.9139 - val_loss: 0.3322 - val_accuracy: 0.8624\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 211s 169ms/step - loss: 0.2626 - accuracy: 0.9113 - val_loss: 0.3495 - val_accuracy: 0.8620\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 211s 169ms/step - loss: 0.2633 - accuracy: 0.9136 - val_loss: 0.3377 - val_accuracy: 0.8632\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 211s 169ms/step - loss: 0.2617 - accuracy: 0.9128 - val_loss: 0.3247 - val_accuracy: 0.8630\n"
     ]
    }
   ],
   "source": [
    "print(\"Custom Model: training custom model on IMDB\")\n",
    "custom_history = custom_model.fit(train_ds, epochs=10, \n",
    "                                  validation_data=valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T20:34:00.938308Z",
     "start_time": "2020-10-01T20:30:48.004399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 193s 123ms/step - loss: 0.3239 - accuracy: 0.8667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3239259719848633, 0.8666800260543823]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T20:34:26.035744Z",
     "start_time": "2020-10-01T20:34:25.895723Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now finetune BERT for a couple of epochs\n",
    "bert.trainable = True\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.BinaryCrossentropy() #from_logits=True)\n",
    "\n",
    "custom_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T20:34:32.704071Z",
     "start_time": "2020-10-01T20:34:32.667872Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "attention_mask (InputLayer)     [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_ids (InputLayer)          [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_type_ids (InputLayer)     [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     ((None, 150, 768), ( 108310272   attention_mask[0][0]             \n",
      "                                                                 input_ids[0][0]                  \n",
      "                                                                 token_type_ids[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_123 (Dropout)           (None, 768)          0           tf_bert_model[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 200)          153800      dropout_123[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_124 (Dropout)           (None, 200)          0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 2)            402         dropout_124[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 108,464,474\n",
      "Trainable params: 108,464,474\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "custom_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T21:06:30.079957Z",
     "start_time": "2020-10-01T20:50:48.879315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Model: Fine-tuning BERT on IMDB\n",
      "Epoch 1/2\n",
      "1250/1250 [==============================] - 470s 376ms/step - loss: 0.0845 - accuracy: 0.9754 - val_loss: 0.4150 - val_accuracy: 0.8874\n",
      "Epoch 2/2\n",
      "1250/1250 [==============================] - 470s 376ms/step - loss: 0.0494 - accuracy: 0.9862 - val_loss: 0.4479 - val_accuracy: 0.8870\n"
     ]
    }
   ],
   "source": [
    "print(\"Custom Model: Fine-tuning BERT on IMDB\")\n",
    "custom_history = custom_model.fit(train_ds, epochs=2, \n",
    "                                  validation_data=valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T21:11:01.840403Z",
     "start_time": "2020-10-01T21:07:45.017076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 196s 126ms/step - loss: 0.5060 - accuracy: 0.8750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5059850215911865, 0.8749600052833557]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "tf23nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "348px",
    "left": "799px",
    "right": "20px",
    "top": "120px",
    "width": "358px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
