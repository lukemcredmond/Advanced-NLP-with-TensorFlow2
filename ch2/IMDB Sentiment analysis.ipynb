{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:03:13.528704Z",
     "start_time": "2020-10-01T07:03:11.616714Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow_datasets\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only if you have a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:03:30.971832Z",
     "start_time": "2020-10-01T07:03:30.919332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "print(get_available_devices()) \n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:04:17.819473Z",
     "start_time": "2020-10-01T07:04:16.967326Z"
    }
   },
   "outputs": [],
   "source": [
    "######## GPU CONFIGS FOR RTX 2070 ###############\n",
    "## Please ignore if not training on GPU       ##\n",
    "## this is important for running CuDNN on GPU ##\n",
    "\n",
    "# check if GPU can be seen by TF\n",
    "tf.config.list_physical_devices('GPU')\n",
    "# enable this line if you wish to see whether GPU/CPU executes\n",
    "# each command\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:04:34.432270Z",
     "start_time": "2020-10-01T07:04:34.425717Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abstract_reasoning, accentdb, aeslc, aflw2k3d, ag_news_subset, ai2_arc, ai2_arc_with_ir, amazon_us_reviews, anli, arc, bair_robot_pushing_small, bccd, beans, big_patent, bigearthnet, billsum, binarized_mnist, binary_alpha_digits, blimp, bool_q, c4, caltech101, caltech_birds2010, caltech_birds2011, cars196, cassava, cats_vs_dogs, celeb_a, celeb_a_hq, cfq, cherry_blossoms, chexpert, cifar10, cifar100, cifar10_1, cifar10_corrupted, citrus_leaves, cityscapes, civil_comments, clevr, clic, clinc_oos, cmaterdb, cnn_dailymail, coco, coco_captions, coil100, colorectal_histology, colorectal_histology_large, common_voice, coqa, cos_e, cosmos_qa, covid19, covid19sum, crema_d, curated_breast_imaging_ddsm, cycle_gan, d4rl_adroit_door, d4rl_adroit_hammer, d4rl_adroit_pen, d4rl_adroit_relocate, d4rl_mujoco_ant, d4rl_mujoco_halfcheetah, d4rl_mujoco_hopper, d4rl_mujoco_walker2d, dart, davis, deep_weeds, definite_pronoun_resolution, dementiabank, diabetic_retinopathy_detection, div2k, dmlab, doc_nli, dolphin_number_word, downsampled_imagenet, drop, dsprites, dtd, duke_ultrasound, e2e_cleaned, efron_morris75, emnist, eraser_multi_rc, esnli, eurosat, fashion_mnist, flic, flores, food101, forest_fires, fuss, gap, geirhos_conflict_stimuli, gem, genomics_ood, german_credit_numeric, gigaword, glue, goemotions, gpt3, gref, groove, gtzan, gtzan_music_speech, hellaswag, higgs, horses_or_humans, howell, i_naturalist2017, imagenet2012, imagenet2012_corrupted, imagenet2012_multilabel, imagenet2012_real, imagenet2012_subset, imagenet_a, imagenet_r, imagenet_resized, imagenet_v2, imagenette, imagewang, imdb_reviews, irc_disentanglement, iris, kddcup99, kitti, kmnist, lambada, lfw, librispeech, librispeech_lm, libritts, ljspeech, lm1b, lost_and_found, lsun, lvis, malaria, math_dataset, mctaco, mlqa, mnist, mnist_corrupted, movie_lens, movie_rationales, movielens, moving_mnist, multi_news, multi_nli, multi_nli_mismatch, natural_questions, natural_questions_open, newsroom, nsynth, nyu_depth_v2, ogbg_molpcba, omniglot, open_images_challenge2019_detection, open_images_v4, openbookqa, opinion_abstracts, opinosis, opus, oxford_flowers102, oxford_iiit_pet, para_crawl, patch_camelyon, paws_wiki, paws_x_wiki, pet_finder, pg19, piqa, places365_small, plant_leaves, plant_village, plantae_k, protein_net, qa4mre, qasc, quac, quickdraw_bitmap, race, radon, reddit, reddit_disentanglement, reddit_tifu, ref_coco, resisc45, rlu_atari, rlu_dmlab_explore_object_rewards_few, rlu_dmlab_explore_object_rewards_many, rlu_dmlab_rooms_select_nonmatching_object, rlu_dmlab_rooms_watermaze, rlu_dmlab_seekavoid_arena01, robonet, rock_paper_scissors, rock_you, s3o4d, salient_span_wikipedia, samsum, savee, scan, scene_parse150, schema_guided_dialogue, scicite, scientific_papers, sentiment140, shapes3d, siscore, smallnorb, snli, so2sat, speech_commands, spoken_digit, squad, stanford_dogs, stanford_online_products, star_cfq, starcraft_video, stl10, story_cloze, summscreen, sun397, super_glue, svhn_cropped, symmetric_solids, tao, ted_hrlr_translate, ted_multi_translate, tedlium, tf_flowers, the300w_lp, tiny_shakespeare, titanic, trec, trivia_qa, tydi_qa, uc_merced, ucf101, vctk, visual_domain_decathlon, voc, voxceleb, voxforge, waymo_open_dataset, web_nlg, web_questions, wider_face, wiki40b, wiki_bio, wiki_table_questions, wiki_table_text, wikiann, wikihow, wikipedia, wikipedia_toxicity_subtypes, wine_quality, winogrande, wmt13_translate, wmt14_translate, wmt15_translate, wmt16_translate, wmt17_translate, wmt18_translate, wmt19_translate, wmt_t2t_translate, wmt_translate, wordnet, wsc273, xnli, xquad, xsum, xtreme_pawsx, xtreme_xnli, yelp_polarity_reviews, yes_no, youtube_vis'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see available tfds data sets\n",
    "\", \".join(tfds.list_builders())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:04:37.653833Z",
     "start_time": "2020-10-01T07:04:37.554098Z"
    }
   },
   "outputs": [],
   "source": [
    "# using TFDS dataset\n",
    "# note: as_supervised converts dicts to tuples\n",
    "imdb_train, ds_info = tfds.load(name=\"imdb_reviews\", split=\"train\", \n",
    "                                with_info=True, as_supervised=True)\n",
    "imdb_test = tfds.load(name=\"imdb_reviews\", split=\"test\", \n",
    "                      as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:04:40.136066Z",
     "start_time": "2020-10-01T07:04:40.130384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='imdb_reviews',\n",
      "    full_name='imdb_reviews/plain_text/1.0.0',\n",
      "    description=\"\"\"\n",
      "    Large Movie Review Dataset.\n",
      "    This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.\n",
      "    \"\"\",\n",
      "    config_description=\"\"\"\n",
      "    Plain text\n",
      "    \"\"\",\n",
      "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
      "    data_path='C:\\\\Users\\\\lukem\\\\tensorflow_datasets\\\\imdb_reviews\\\\plain_text\\\\1.0.0',\n",
      "    download_size=80.23 MiB,\n",
      "    dataset_size=129.83 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
      "        'text': Text(shape=(), dtype=tf.string),\n",
      "    }),\n",
      "    supervised_keys=('text', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=25000, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=25000, num_shards=1>,\n",
      "        'unsupervised': <SplitInfo num_examples=50000, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
      "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
      "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
      "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
      "      month     = {June},\n",
      "      year      = {2011},\n",
      "      address   = {Portland, Oregon, USA},\n",
      "      publisher = {Association for Computational Linguistics},\n",
      "      pages     = {142--150},\n",
      "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:04:43.390551Z",
     "start_time": "2020-10-01T07:04:43.326285Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string) \n",
      " tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for example, label in imdb_train.take(1):\n",
    "    print(example, '\\n', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:04:54.364677Z",
     "start_time": "2020-10-01T07:04:47.490172Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the default tokenizer settings\n",
    "tokenizer = tfds.deprecated.text.Tokenizer()\n",
    "\n",
    "vocabulary_set = set()\n",
    "MAX_TOKENS = 0\n",
    "\n",
    "for example, label in imdb_train:\n",
    "  some_tokens = tokenizer.tokenize(example.numpy())\n",
    "  if MAX_TOKENS < len(some_tokens):\n",
    "        MAX_TOKENS = len(some_tokens)\n",
    "  vocabulary_set.update(some_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:05:55.711915Z",
     "start_time": "2020-10-01T07:05:55.538523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93931 2525\n"
     ]
    }
   ],
   "source": [
    "imdb_encoder = tfds.deprecated.text.TokenTextEncoder(vocabulary_set, \n",
    "                                                   tokenizer=tokenizer)\n",
    "vocab_size = imdb_encoder.vocab_size\n",
    "\n",
    "print(vocab_size, MAX_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:05:57.667656Z",
     "start_time": "2020-10-01T07:05:57.624948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string)\n",
      "This was an absolutely terrible movie Don t be lured in by Christopher Walken or Michael Ironside Both are great actors but this must simply be their worst role in history Even their great acting could not redeem this movie s ridiculous storyline This movie is an early nineties US propaganda piece The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions Maria Conchita Alonso appeared phony and her pseudo love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning I am disappointed that there are movies like this ruining actor s like Christopher Walken s good name I could barely sit through it\n"
     ]
    }
   ],
   "source": [
    "# Lets verify tokenization and encoding works\n",
    "for example, label in imdb_train.take(1):\n",
    "    print(example)\n",
    "    encoded = imdb_encoder.encode(example.numpy())\n",
    "    print(imdb_encoder.decode(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:05:59.963100Z",
     "start_time": "2020-10-01T07:05:59.954134Z"
    }
   },
   "outputs": [],
   "source": [
    "# transformation fucntions to be used with the dataset\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "def encode_pad_transform(sample):\n",
    "    encoded = imdb_encoder.encode(sample.numpy())\n",
    "    pad = sequence.pad_sequences([encoded], padding='post', \n",
    "                                 maxlen=150)\n",
    "    return np.array(pad[0], dtype=np.int64)  \n",
    "\n",
    "\n",
    "def encode_tf_fn(sample, label):\n",
    "    encoded = tf.py_function(encode_pad_transform, \n",
    "                                       inp=[sample], \n",
    "                                       Tout=(tf.int64))\n",
    "    encoded.set_shape([None])\n",
    "    label.set_shape([])\n",
    "    return encoded, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:06:00.776090Z",
     "start_time": "2020-10-01T07:06:00.703019Z"
    }
   },
   "outputs": [],
   "source": [
    "# test the transformation on a small subset\n",
    "subset = imdb_train.take(10)\n",
    "tst = subset.map(encode_tf_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:06:01.845914Z",
     "start_time": "2020-10-01T07:06:01.641109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[88995 69667 88990  1707 25264 53864 30424 38705 43359 29772 91140 25947\n",
      " 70621 17761 26734 78285 44754 92453 62305 30438 18154 61228 69805  7446\n",
      " 68264 43359 75752 43540  6642 91140 31806 82836 75752 30438 37908  4941\n",
      " 84805 77578 69805 53864 43166 54698 82791 88995 53864   995 88990 71927\n",
      " 46216 16065  2830 88364 67858 27828 69001 67242 81867 37041 39488  5183\n",
      " 35505 31013 81867 26803 75752 32728 55124 45418  9683 21095 34848 43968\n",
      " 50487 90516 39766 48623 82884 23831 35484 17761 69667 46082 61228 50950\n",
      " 69001 50522   420 91140 50950 53864 92633 69667 61114 68214 68832 82485\n",
      " 48694  4819 76842 56931 92633 43254 62305 54963 66876 69805 28104 18715\n",
      " 43166 66876 70621 17761 43166 71226 44483  4819  4941 66703 78838 66148\n",
      " 54595     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0], shape=(150,), dtype=int64) tf.Tensor(0, shape=(), dtype=int64)\n",
      "This was an absolutely terrible movie Don t be lured in by Christopher Walken or Michael Ironside Both are great actors but this must simply be their worst role in history Even their great acting could not redeem this movie s ridiculous storyline This movie is an early nineties US propaganda piece The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions Maria Conchita Alonso appeared phony and her pseudo love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning I am disappointed that there are movies like this ruining actor s like Christopher Walken s good name I could barely sit through it\n"
     ]
    }
   ],
   "source": [
    "for review, label in tst.take(1):\n",
    "    print(review, label)\n",
    "    print(imdb_encoder.decode(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:06:03.379137Z",
     "start_time": "2020-10-01T07:06:03.359510Z"
    }
   },
   "outputs": [],
   "source": [
    "#now tokenize/encode/pad all training\n",
    "# and testing data\n",
    "encoded_train = imdb_train.map(encode_tf_fn,\n",
    "                               num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "encoded_test = imdb_test.map(encode_tf_fn,\n",
    "                             num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:06:19.472832Z",
     "start_time": "2020-10-01T07:06:19.467992Z"
    }
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = imdb_encoder.vocab_size # len(chars)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 64\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 64\n",
    "\n",
    "#batch size\n",
    "BATCH_SIZE=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:06:21.557245Z",
     "start_time": "2020-10-01T07:06:21.550589Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model_lstm(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.LSTM(rnn_units),\n",
    "    # introduce a dropout layer after the LSTM layer\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:06:26.612154Z",
     "start_time": "2020-10-01T07:06:25.825486Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (100, None, 64)           6011584   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (100, 64)                 33024     \n",
      "                                                                 \n",
      " dense (Dense)               (100, 1)                  65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,044,673\n",
      "Trainable params: 6,044,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model_lstm(\n",
    "  vocab_size = vocab_size,\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:06:31.817335Z",
     "start_time": "2020-10-01T07:06:31.799369Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:06:37.541747Z",
     "start_time": "2020-10-01T07:06:37.536302Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prefetch for performance\n",
    "encoded_train_batched = encoded_train.batch(BATCH_SIZE).prefetch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:10:27.683509Z",
     "start_time": "2020-10-01T07:06:40.292699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "250/250 [==============================] - 88s 336ms/step - loss: 0.4083 - accuracy: 0.8051 - precision: 0.7877 - recall: 0.8354\n",
      "Epoch 2/10\n",
      "250/250 [==============================] - 82s 327ms/step - loss: 0.1685 - accuracy: 0.9396 - precision: 0.9404 - recall: 0.9386\n",
      "Epoch 3/10\n",
      "250/250 [==============================] - 84s 337ms/step - loss: 0.1038 - accuracy: 0.9647 - precision: 0.9655 - recall: 0.9638\n",
      "Epoch 4/10\n",
      "250/250 [==============================] - 87s 348ms/step - loss: 0.0814 - accuracy: 0.9722 - precision: 0.9726 - recall: 0.9717\n",
      "Epoch 5/10\n",
      "250/250 [==============================] - 83s 330ms/step - loss: 0.0546 - accuracy: 0.9815 - precision: 0.9828 - recall: 0.9802\n",
      "Epoch 6/10\n",
      "250/250 [==============================] - 63s 252ms/step - loss: 0.0373 - accuracy: 0.9884 - precision: 0.9897 - recall: 0.9871\n",
      "Epoch 7/10\n",
      "250/250 [==============================] - 83s 331ms/step - loss: 0.0477 - accuracy: 0.9834 - precision: 0.9833 - recall: 0.9834\n",
      "Epoch 8/10\n",
      "250/250 [==============================] - 87s 347ms/step - loss: 0.0351 - accuracy: 0.9885 - precision: 0.9889 - recall: 0.9881\n",
      "Epoch 9/10\n",
      "250/250 [==============================] - 88s 353ms/step - loss: 0.0251 - accuracy: 0.9922 - precision: 0.9912 - recall: 0.9932\n",
      "Epoch 10/10\n",
      "250/250 [==============================] - 90s 359ms/step - loss: 0.0225 - accuracy: 0.9928 - precision: 0.9924 - recall: 0.9931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22f960ea160>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(encoded_train_batched, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:10:55.965511Z",
     "start_time": "2020-10-01T07:10:34.119008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 27s 100ms/step - loss: 0.7815 - accuracy: 0.8352 - precision: 0.8187 - recall: 0.861017s - loss: 0.8059 - accuracy: 0.8326 - precision: 0.8134 -  - ETA: 16s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7814833521842957,\n",
       " 0.8352000117301941,\n",
       " 0.8187281489372253,\n",
       " 0.8610399961471558]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(encoded_test.batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:11:23.892847Z",
     "start_time": "2020-10-01T07:11:23.887880Z"
    }
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = imdb_encoder.vocab_size # len(chars)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 128\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 64\n",
    "\n",
    "#batch size\n",
    "BATCH_SIZE=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:11:25.201887Z",
     "start_time": "2020-10-01T07:11:25.192055Z"
    }
   },
   "outputs": [],
   "source": [
    "dropout=0.2\n",
    "def build_model_bilstm(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.Dropout(dropout),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_units, return_sequences=True, dropout=0.25)),\n",
    "    tf.keras.layers.Dropout(dropout),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_units, dropout=0.25)),\n",
    "    tf.keras.layers.Dropout(dropout),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:11:29.441086Z",
     "start_time": "2020-10-01T07:11:26.487139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (50, None, 128)           12023168  \n",
      "                                                                 \n",
      " dropout (Dropout)           (50, None, 128)           0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (50, None, 128)          98816     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (50, None, 128)           0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (50, 128)                98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (50, 128)                 0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (50, 1)                   129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,220,929\n",
      "Trainable params: 12,220,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bilstm = build_model_bilstm(\n",
    "  vocab_size = vocab_size,\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "bilstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:11:29.575709Z",
     "start_time": "2020-10-01T07:11:29.562501Z"
    }
   },
   "outputs": [],
   "source": [
    "bilstm.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:11:32.251708Z",
     "start_time": "2020-10-01T07:11:32.245767Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_train_batched = encoded_train.batch(BATCH_SIZE).prefetch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:17:29.418810Z",
     "start_time": "2020-10-01T07:11:34.295046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "500/500 [==============================] - 419s 809ms/step - loss: 0.3729 - accuracy: 0.8278 - precision: 0.8245 - recall: 0.8329\n",
      "Epoch 2/5\n",
      "500/500 [==============================] - 275s 550ms/step - loss: 0.1532 - accuracy: 0.9456 - precision: 0.9461 - recall: 0.9450\n",
      "Epoch 3/5\n",
      "500/500 [==============================] - 283s 566ms/step - loss: 0.0742 - accuracy: 0.9750 - precision: 0.9757 - recall: 0.9742\n",
      "Epoch 4/5\n",
      "500/500 [==============================] - 276s 552ms/step - loss: 0.0523 - accuracy: 0.9819 - precision: 0.9823 - recall: 0.9815\n",
      "Epoch 5/5\n",
      "500/500 [==============================] - 275s 550ms/step - loss: 0.0384 - accuracy: 0.9869 - precision: 0.9876 - recall: 0.9862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22f960eaa00>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm.fit(encoded_train_batched, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:17:50.858013Z",
     "start_time": "2020-10-01T07:17:29.532181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 17s 35ms/step - loss: 0.6753 - accuracy: 0.8384 - precision: 0.8459 - recall: 0.8274\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6753188967704773,\n",
       " 0.8383600115776062,\n",
       " 0.8459147810935974,\n",
       " 0.8274400234222412]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm.evaluate(encoded_test.batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "tf23nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
