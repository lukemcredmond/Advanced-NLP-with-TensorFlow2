{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 Understanding Sentiment\n",
    "\n",
    "## Natural language understanding\n",
    "NLU enables the processing of unstructured text and extracts meaning and critical pieces of information that are actionable\n",
    "\n",
    "Sentiment analysis of a sentence becomes possible after understanding the sentence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Bi-directional LSTMs â€“ BiLSTMs\n",
    "LSTMs are one of the styles of recurrent neural networks\n",
    "\n",
    "using the output generated after processing the previous item in the sequence along with the current item to generate the next output\n",
    "\n",
    "### RNN building blocks\n",
    "\n",
    "### Long short-term memory (LSTM) networks\n",
    "- Cell value or memory of the network, also referred to as the cell, which stores accumulated knowledge\n",
    "- Input gate, which controls how much of the input is used in computing the new cell value\n",
    "- Output gate, which determines how much of the cell value is used in the output\n",
    "- Forget gate, which determines how much of the current cell value is used for updating the cell value\n",
    "\n",
    "### Gated recurrent units (GRUs)\n",
    "GRUs were invented in 2014. They are simpler than LSTMs:\n",
    "Compared to the LSTM, it has fewer gates.\n",
    "LSTMs have shown superior performance\n",
    "\n",
    "### Sentiment classification with LSTMs\n",
    "[~Action~]\n",
    "Need to see how tfds.load works !!!!!!\n",
    "we need to load the data and split into train and test data.\n",
    "\n",
    "this is how that can be achived \n",
    "imdb_train, ds_info = tfds.load(name=\"imdb_reviews\", split=\"train\", with_info=True, as_supervised=True)\n",
    "imdb_test = tfds.load(name=\"imdb_reviews\", split=\"test\", as_supervised=True)\n",
    "\n",
    "[~Action~]\n",
    "i am not sure how it does the split here!!!!!!!!\n",
    "need to review abit more.\n",
    "\n",
    "next we tokenise the words using tfds.features.text.Tokenizer()\n",
    "This is deprecated so for the mean time i am using tfds.deprecated.text.Tokenizer !!!!!!! need to research this.\n",
    "\n",
    "TokenTextEncoder is one of three out-of-the-box encoders that are provided in tfds\n",
    "this is also in the deprecated lib so need to research this replacement also.\n",
    "\n",
    "\"This encoder expects that the tokenizer object provides a tokenize() and a join() method. If you want to use StanfordNLP or some other tokenizer as discussed in the previous chapter, all you need to do is to wrap the StanfordNLP interface in a custom object and implement methods to split the text into tokens and join the tokens back into a string\"\n",
    "\n",
    "LSTM expects seqiences of equal lengths\n",
    "\n",
    "imdb_encoder.save_to_file(\"reviews_vocab\")\n",
    "enc = tfds.features.text.TokenTextEncoder.load_from_file(\"reviews_vocab\")\n",
    "enc.decode(enc.encode(\"Good case. Excellent value.\"))\n",
    "\n",
    "filter() can remove certain types of data from the dataset. It can be used to filter out reviews above or below a certain length, or separate out positive and negative examples to construct a more balanced dataset\n",
    "\n",
    "shuffle()  shuffles the data between training epochs\n",
    "\n",
    "batch() batches data for training\n",
    "\n",
    "\n",
    "LSTM model with embeddings\n",
    "TensorFlow and Keras make it trivial to instantiate an LSTM-based model \"tf.keras.layers.LSTM(rnn_units)\"\n",
    "Here, the rnn_units parameter determines how many LSTMs are strung together in one layer\n",
    "\n",
    "[~Action~]\n",
    "Add a \" introduce a dropout layer after the LSTM layer\" into the model!!!!!!\n",
    "\n",
    "\n",
    "BiLSTM model\n",
    "Building BiLSTMs is easy in TensorFlow. All that is required is a one-line change in the model definition\n",
    "\n",
    "\"tf.keras.layers.LSTM(rnn_units)\" -> \"tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_units)),\"\n",
    "\n",
    "\n",
    "If you are working with right-to-left languages such as Arabic and Hebrew, please feed the tokens right to left. It is important to understand the direction the next word or token comes from. If you are using a BiLSTM, then the direction may not matter as much."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
