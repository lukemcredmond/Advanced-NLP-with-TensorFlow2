{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Getting Started\n",
    "\n",
    "Code can be found at: https://github.com/PacktPublishing/Advanced-NLP-with-TensorFlow-2\n",
    "\n",
    "\n",
    "\n",
    "# Chapter 1 Essentials Of NLP\n",
    "\n",
    "## The typical text processing workflow\n",
    "- Data collection and labeling\n",
    "- Text normalization, including case normalization, text tokenization, stemming, and lemmatization\n",
    "    - Modeling datasets that have been text normalized\n",
    "    - Vectorizing text\n",
    "    - Modeling datasets with vectorized text\n",
    "\n",
    "\n",
    "Scraping/Data Collection -> Labeling -> [Tex Normalization -> Vectorization / Featurizing -> Modeling]* repeat these iterations\n",
    "\n",
    "\n",
    "## Getting Data\n",
    "libraries such as scrapy or Beautiful Soup to scrape data from the web\n",
    "\n",
    "University of California, Irvine, is a great source of machine learning datasets. \n",
    "You can see all the datasets they provide by visiting http://archive.ics.uci.edu/ml/datasets.php. \n",
    "Specifically for NLP, you can see some publicly available datasets on https://github.com/niderhoff/nlp-datasets.\n",
    "\n",
    "## Tools\n",
    "colab.research.google.com\n",
    "\n",
    "\n",
    "## Text normalization\n",
    "- case normalization, \n",
    "- tokenization and stop word removal, \n",
    "- Parts-of-Speech (POS) tagging, \n",
    "- stemming\n",
    "\n",
    "### case normalization \n",
    "use upper- and lowercase letters, \n",
    "all letters are converted to the same case, \n",
    "remove punctuation, \n",
    "\n",
    "open a file using io.open and split the file on ('\\n') to get an arrany of lines.\n",
    "create an array of objects\n",
    "each object stores the label and its data.\n",
    "loop through the lines and split on the sperator\n",
    "detect the label \n",
    "set the label value in the object.\n",
    "set the text value in the object\n",
    "add object to array\n",
    "\n",
    "using pandas\n",
    "convert the array into a dataframe\n",
    "import pandas as pd \n",
    "df = pd.DataFrame(spam_dataset, columns=['Spam', 'Message'])\n",
    "df['Capitals'] = df['Message'].apply(num_capitals)\n",
    "df['Punctuation'] = df['Message'].apply(num_punctuation)\n",
    "df['Length'] = df['Message'].apply(message_length)\n",
    "\n",
    "next you will need to split the dataset into train and test sets\n",
    "train=df.sample(frac=0.8,random_state=42)\n",
    "test=df.drop(train.index)\n",
    "x_train = train[['Length', 'Capitals', 'Punctuation']]\n",
    "y_train = train[['Spam']]\n",
    "x_test = test[['Length', 'Capitals', 'Punctuation']]\n",
    "y_test = test[['Spam']]\n",
    "\n",
    "build the model specifing the layers you want.\n",
    "see code example, need to check up on the tf.keras.layers.Dense options\n",
    "\n",
    "### Tokenization\n",
    "chopping up text into words\n",
    "keep getting the following error\n",
    "KeyError: \"Constituency parser not trained with tag 'GW'\" -> pip install stanza==1.2.3 instead\n",
    "from what i can tell i think its an issue with the lib '!pip install stanfordnlp'\n",
    "they have a fix might try install a older version\n",
    "\n",
    "\n",
    "Modeling tokenized data\n",
    "This model can be trained like so:\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=10)\n",
    "\n",
    "- Stop words removal\n",
    "uses a library to remove words\n",
    "rebuild a model and seen the reduction in words.\n",
    "\n",
    "- Part of Speech tagging\n",
    "tagging words with a {verbs, adverbs, nouns, and adjectives} category\n",
    "\n",
    "| Tag | Class | Examples |\n",
    "| ADJ | Adjective: Usually describes a noun. Separate tags are used for comparatives and superlatives. | Great, pretty |\n",
    "| ADP | Adposition: Used to modify an object such as a noun, pronoun, or phrase; for example, \"Walk up the stairs.\" Some languages like English use prepositions while others such as Hindi and Japanese use postpositions. | Up, inside |\n",
    "| ADV | Adverb: A word or phrase that modifies or qualifies an adjective, verb, or another adverb. | Loudly, often |\n",
    "| AUX | Auxiliary verb: Used in forming mood, voice, or tenses of other verbs. | Will, can, may |\n",
    "| CCONJ | Co-ordinating conjunction: Joins two phrases, clauses, or sentences. | And, but, that |\n",
    "| INTJ | Interjection: An exclamation, interruption, or sudden remark. | Oh, uh, lol |\n",
    "| NOUN | Noun: Identifies people, places, or things. | Office, book |\n",
    "| NUM | Numeral: Represents a quantity. | Six, nine |\n",
    "| DET | Determiner: Identifies a specific noun, usually as a singular. | A, an, the |\n",
    "| PART | Particle: Parts of speech outside of the main types. | To, n't |\n",
    "| PRON | Pronoun: Substitutes for other nouns, especially proper nouns. | She, her |\n",
    "| PROPN | Proper noun: A name for a specific person, place, or thing. | Gandhi, US |\n",
    "| PUNCT | Different punctuation symbols. | , ? / |\n",
    "| SCONJ | Subordinating conjunction: Connects independent clause to a dependent clause. | Because, while |\n",
    "| SYM | Symbols including currency signs, emojis, and so on. | $, #, % :) |\n",
    "| VERB | Verb: Denotes action or occurrence. | Go, do |\n",
    "| X | Other: That which cannot be classified elsewhere. | Etc, 4. (a numbered list bullet) |\n",
    "\n",
    "\n",
    "\n",
    "So Basically we have gotten all the text\n",
    "Taken Count of the following for known spam and non spam documents.\n",
    "Capitals\n",
    "Punctuation\n",
    "Length\n",
    "Words\n",
    "Words_NoPunct\n",
    "Punct\n",
    "\n",
    "\n",
    "- Stemming and lemmatization\n",
    "\"Stemming is aimed at reducing vocabulary and aid understanding of morphological processes. This helps people understand the morphology of words and reduce size of corpus\"\n",
    "\n",
    "after stemming using the porters algorithm then it would be \n",
    "\n",
    "\"Stem is aim at reduce vocabulari and aid understand of morpholog process . Thi help peopl understand the morpholog of word and reduc size of corpu .\"\n",
    "\n",
    "\n",
    "### Vectorizing text\n",
    "- Count-based vectorization\n",
    "Each unique word appearing in the corpus is assigned a column in the vocabulary\n",
    "Each document, which would correspond to individual messages in the spam example, is assigned a row. The counts of the words appearing in that document are entered in the relevant cell corresponding to the document and the word. With n unique documents containing m unique words, this results in a matrix of n rows by m columns. Consider a corpus like so:\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "vectorizer.get_feature_names()\n",
    "\n",
    "This can be used to run sentence into arrays of equal lenth with a word count in a index\n",
    "X = array([[0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 2, 0, 0],\n",
    "       [1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0],\n",
    "       [1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1]])\n",
    "with this then you can run a cosine similarity to determine with sentence is closer\n",
    "\n",
    "query = vectorizer.transform([\"apple and bananas\"])\n",
    "cosine_similarity(X, query)\n",
    "\n",
    "- Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    " If a word is rare, we want to give it a higher weight, as it may contain more information than a common word\n",
    " Fortunately, sklearn provides methods to compute TF-IDF.\n",
    " from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "tfidf = transformer.fit_transform(X.toarray())\n",
    "\n",
    "pd.DataFrame(tfidf.toarray(), \n",
    "             columns=vectorizer.get_feature_names())\n",
    "\n",
    "- Word Vectors\n",
    "Taking an example of a window of five words, the word in the center is used to predict two words before and two words after it.\n",
    "trying to find words similar.\n",
    "\n",
    "queen = king-man+woman\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
