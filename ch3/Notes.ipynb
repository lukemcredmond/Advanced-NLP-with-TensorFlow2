{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 Named Entity Recognition (NER) With BiLSTMs, CRFs & Viterbi Decoding\n",
    "\n",
    "Named Entity Recognition (NER) is One of the fundamental building blocks of NLU\n",
    "The names of people, companies, products, and quantities can be tagged in a piece of text with NER, which is very useful in chatbot among others\n",
    "\n",
    "## Named Entity Recognition\n",
    "\n",
    "the objective of an NER model is to locate and classify text tokens as named entities in categories such as people's names, organizations and companies, physical locations, quantities, monetary quantities, times, dates, and even protein or DNA sequences\n",
    "\n",
    "| Type | Example Tag | Example |\n",
    "| Person | PER | Gregory went to the castle. |\n",
    "| Organization | ORG | WHO just issued an epidemic advisory. |\n",
    "| Location | LOC | She lives in Seattle. |\n",
    "| Money | MONEY | You owe me twenty dollars. |\n",
    "| Percentage | PERCENT | Stocks have risen 10% today. |\n",
    "| Date | DATE | Let's meet on Wednesday. |\n",
    "| Time | TIME | Is it 5 pm already? |\n",
    "\n",
    "Data Set Examples\n",
    " random collection -> https://github.com/juand-r/entity-recognition-datasets\n",
    " re3d -> The Defence Science Technology Laboratory (https://github.com/dstl/re3d)\n",
    "\n",
    "There are a few different ways to build an NER model\n",
    "- Part of Speech (POS) tagging are applicable, The POS of a word and its neighboring words are the most straightforward features to add\n",
    "- Word shape features that model lowercase letters can add a lot of information, principally because a lot of the entity types deal with proper nouns, such as those for people and organizations\n",
    "\n",
    "Another vital feature involves checking a word in a gazetteer.\n",
    "A gazetteer is like a database of important geographical entities http://geonames.org/\n",
    "\n",
    "### The GMB data set\n",
    "This dataset is not considered a gold standard. This means that this data set is built using automatic tagging software, followed by human raters updating subsets of the data\n",
    "\n",
    "geo = Geographical entity\n",
    "org = Organization\n",
    "per = Person\n",
    "gpe = Geopolitical entity\n",
    "tim = Time indicator\n",
    "art = Artifact\n",
    "eve = Event\n",
    "nat = Natural phenomenon\n",
    "\n",
    "### Loading the data\n",
    "\n",
    "here there was alot about download the data from a site, and extracting the data from the directories.\n",
    "\n",
    "### Normalizing and vectorizing data\n",
    "For this section, pandas and numpy methods will be used. \n",
    "\n",
    "- The first step is to load the contents of the processed files into one DataFrame:\n",
    "- The next step is Both the text and NER tags need to be tokenized and encoded into numbers for use in training\n",
    "- The last step above is to ensure that shapes are correct before moving to the next step.\n",
    "- There is an additional step that needs to be performed on the labels. Since there are multiple labels, each label token needs to be one-hot encoded\n",
    "\n",
    "Now, we are ready to build and train a model.\n",
    "\n",
    "### A BiLSTM model\n",
    "build the model using layers\n",
    "split the data into train and test\n",
    "train the model\n",
    "and evaluate the model\n",
    "\n",
    "### Conditional random fields (CRFs)\n",
    "consider a subset of NER tags: O, B-Per, I-Per, B-Geo, and I-Geo.\n",
    "here we use a wieght from on tag to the other and build a matrix of these weights.\n",
    "from the weights we can determine the likelyhood of one word be followed by another word based on that words tags\n",
    "\n",
    "example I-Org to B-Org has a weight of -1.38, implying that this transition is extremely unlikely to happen.\n",
    "\n",
    "\n",
    "### NER with BiLSTM and CRFs\n",
    "here we have to build a model layer using keras, create a custom class that passes the output from one layer as the input into another layer\n",
    "\n",
    "example:\n",
    "        inputs = self.embedding(text)\n",
    "        bilstm = self.biLSTM(inputs)\n",
    "        logits = self.dense(bilstm)\n",
    "        outputs = self.crf(logits, seq_lengths, training)\n",
    "\n",
    "### Viterbi decoding\n",
    "The Viterbi algorithm is used to take the predictions for each word in the sequence and apply a maximization algorithm so that the output sequence has the highest likelihood"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
